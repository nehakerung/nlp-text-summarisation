{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0ca1249",
   "metadata": {},
   "source": [
    "# Text Summarisation Model\n",
    "\n",
    "Here data is loaded, BART model is used to summarise news articles as well as translate them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9a7d46",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "If you are using google colab, please uncomment this and install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39a6444d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install if on notebook\n",
    "# pip install spacy\n",
    "# pip install transformers\n",
    "# pip install datasets\n",
    "# pip install random\n",
    "# pip install evaulate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc27131",
   "metadata": {},
   "source": [
    "If you are using your local machine, please install the required packages above. Given pip and python are installed; you can run `pip instaall reg`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2706b92b",
   "metadata": {},
   "source": [
    "## 1. Text Pre-processing\n",
    "\n",
    "As text summarisation models require alot of the data to be kept in, text pre-processing function is very minimal. Because of this, i chose Rejex to clean the data Rejex is usead as it does not require much and meets the job, SPACY and nltk is more advanced but uneccessary for this job hence I chose rejex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12762e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def pre_process_txt(text):\n",
    "    \"\"\"Pre-process text by removing unwanted characters and formatting.\"\"\"\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+\", \"\", text)\n",
    "\n",
    "    # Remove non-text characters but keep punctuation marks\n",
    "    text = re.sub(r\"[^\\w\\s\\.,!?;:'\\\"-]\", \" \", text)\n",
    "\n",
    "    # Collapse extra spaces\n",
    "    cleaned_text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aea565b",
   "metadata": {},
   "source": [
    "## 2. Accessing the dataset\n",
    "\n",
    "Open the files uploaded to be read. Also applying text pre-processing function.\n",
    "Try and except in case it failes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f65534a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def load_dataset(base_path):\n",
    "    articles_dir = Path(base_path) / \"Articles\"\n",
    "    summary_dir = Path(base_path) / \"Summary\"\n",
    "\n",
    "    if not articles_dir.exists() or not summary_dir.exists():\n",
    "        raise FileNotFoundError(\"Articles or Summary directory not found.\")\n",
    "\n",
    "    # Get all article files and sort them\n",
    "    article_files = sorted(articles_dir.glob(\"*.txt\"))\n",
    "\n",
    "    dataset = []\n",
    "    \n",
    "    for article_path in article_files:\n",
    "        try:\n",
    "            # Get corresponding summary file\n",
    "            file_id = article_path.stem  # e.g., \"001\" from \"001.txt\"\n",
    "            summary_path = summary_dir / f\"{file_id}.txt\"\n",
    "\n",
    "            # Skip if summary does not exist\n",
    "            if not summary_path.exists():\n",
    "                print(f\"Warning: No summary found for {file_id}\")\n",
    "                continue\n",
    "\n",
    "            # Read both files\n",
    "            with open(article_path, 'r', encoding='utf-8') as f:\n",
    "                article = f.read()\n",
    "                article = pre_process_txt(article)\n",
    "            with open(summary_path, 'r', encoding='utf-8') as f:\n",
    "                summary = f.read()\n",
    "                summary = pre_process_txt(summary)\n",
    "\n",
    "            dataset.append({\n",
    "                'id': file_id,\n",
    "                'article': article,\n",
    "                'summary': summary\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_id}: {e}\")\n",
    "\n",
    "    print(f\"Loaded {len(dataset)} article-summary pairs\")\n",
    "    # Knowing the maximum lengths of articles can help inform model choices and parameters\n",
    "    print(f\"Max article length: {max(d['article_length'] for d in dataset)} tokens\")\n",
    "    # Calculate max and min lengths which will be inform parameters to use for model training\n",
    "    print(f\"Max summary length: {max(d['summary_length'] for d in dataset)} tokens\")\n",
    "    print(f\"Min summary length: {min(d['summary_length'] for d in dataset)} tokens\")\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db078b9",
   "metadata": {},
   "source": [
    "**Please confirm BASE_PATH variable here!** i.e. {BASE_PATH}/Articles or {BASE_PATH}/Summary.\n",
    "\n",
    "Some IDEs may require a `/` at the beginning as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b6e6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BASE_PATH=\"dataset\"\n",
    "\n",
    "data = load_dataset(BASE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f342e0fe",
   "metadata": {},
   "source": [
    "**Use the min and max length to inform the model later on**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecb2cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 390\n",
    "MIN_LEN = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87774a4d",
   "metadata": {},
   "source": [
    "Article length is not noted as the `facebook/bart-large` models, including variations like `bart-large-mnli` and `bart-large-cnn` (which I will be using), are designed to handle a maximum input sequence length of 1024 tokens. This is already alot, if your dataset has even more tokens than 1024, a different model may be needed or a change in dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04c7e13",
   "metadata": {},
   "source": [
    "## 4: Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15ac581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments\n",
    "from datasets import Dataset, DatasetDict\n",
    "import random\n",
    "\n",
    "# Shuffle for randomness\n",
    "random.seed(42)  # For reproducibility\n",
    "random.shuffle(data)\n",
    "\n",
    "# Calculate split indices\n",
    "total = len(data)\n",
    "train_end = int(total * 0.8)  # 80% train\n",
    "test_end = train_end + int(total * 0.1)  # 10% test\n",
    "# Remaining 10% for validation\n",
    "\n",
    "# Split the data\n",
    "train_items = data[:train_end]\n",
    "val_items = data[train_end:test_end]\n",
    "test_items = data[test_end:]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"article\": [item['article'] for item in train_items],\n",
    "    \"summary\": [item['summary'] for item in train_items],\n",
    "})\n",
    "\n",
    "val_dataset = Dataset.from_dict({\n",
    "    \"article\": [item['article'] for item in val_items],\n",
    "    \"summary\": [item['summary'] for item in val_items],\n",
    "})\n",
    "\n",
    "test_dataset = Dataset.from_dict({\n",
    "    \"article\": [item['article'] for item in test_items],\n",
    "    \"summary\": [item['summary'] for item in test_items],\n",
    "})\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset,\n",
    "    \"test\": test_dataset,\n",
    "})\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1a1621",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/bart-large-cnn\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "def preprocess_func(examples):\n",
    "  inputs = examples[\"article\"]\n",
    "  targets = examples[\"summary\"]\n",
    "  model_inputs = tokenizer(inputs, max_length=1024, truncation=True, padding=\"max_length\")\n",
    "\n",
    "  with tokenizer.as_target_tokenizer():\n",
    "    labels = tokenizer(targets, max_length=64, truncation=True, padding=\"max_length\")\n",
    "\n",
    "  model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "  return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset_dict.map(preprocess_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a723eca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bart\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir='./logs',\n",
    "    save_total_limit=1,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"./bart-large-cnn-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62aa5a54",
   "metadata": {},
   "source": [
    "# BART\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f530ff91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To store generated and reference summaries\n",
    "generated_summary = []\n",
    "reference_summary = []\n",
    "\n",
    "\n",
    "for item in dataset_dict[\"test\"]:\n",
    "    article = item[\"article\"]\n",
    "    summary = item[\"summary\"]\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        article,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=1024,\n",
    "        truncation=True # WHAT THIS MEAN\n",
    "    ).to(model.device)\n",
    "\n",
    "    # Generate Summary\n",
    "\n",
    "    summary_ids = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    num_beams=6, # this\n",
    "    no_repeat_ngram_size=3, # this\n",
    "    length_penalty=1.0,\n",
    "    max_length=MAX_LEN,\n",
    "    min_length=MIN_LEN,)\n",
    "    # TODO: Experiment with generation parameters\n",
    "    # no_repeat_ngram_size=2,  # Avoid repetition\n",
    "    # temperature=0.8,  # Add some randomness\n",
    "    # do_sample=False,  # Use greedy/beam search)\n",
    "    # Decode the summary\n",
    "    gen_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    generated_summary.append(gen_summary)\n",
    "    reference_summary.append(summary)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15df6bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "rouge = load(\"rouge\")\n",
    "results = rouge.compute(\n",
    "    predictions=generated_summary,\n",
    "    references=reference_summary\n",
    ")\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5c82ac",
   "metadata": {},
   "source": [
    "{'rouge1': np.float64(0.35611881813181834), 'rouge2': np.float64(0.2286644241575414), 'rougeL': np.float64(0.25650589793161893), 'rougeLsum': np.float64(0.2547639736749432)}\n",
    "Rouge score! btw"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
